\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}
Básicamente como he hecho las cosas, por que las he hecho y en que orden. Que me costó mas de mi aprendizaje, menos, por que fueron distintos, cuales son equivalentes y conceptos que no son lo mismo realmente. Documentar por qué configuro, que configuro, que fin tiene documentar la otra parte.

En esta sección se muestra un resumen de los servicios utilizados a lo largo del proyecto en ambas plataformas, así como una descripción detallada de la configuración del entorno. 

\section{Inicio del proyecto}\label{inicio-del-proyecto}
Este proyecto surge con el afán de conocer y comprender las plataformas de AWS y GCP, las cuales ofrecen multitud de servicios en la nube para realizar diferentes tareas. 

Inicialmente, consideramos configurar un chatbot simple utilizando OpenAI que pudiera entrenarse en base a los documentos de texto recibidos y responder preguntas derivadas desde esos documentos.  Sin embargo, encontramos varias limitaciones, como la incapacidad actual de modelos como ChatGPT para entrenarse directamente a partir de documentos proporcionados por el usuario en tiempo real y responder a preguntas específicas sobre esos documentos. 

Finalmente, tras consultar con el Dr., decidimos explorar una arquitectura más compleja. Así fue como llegamos a evaluar y utilizar las plataformas de AWS y GCP, aprovechando los diversos servicios que ofrecen. Esto nos permitió diseñar y crear un chatbot personalizado, en el cual podemos restringir el flujo de conversación y controlar las respuestas.

\section{Metodologías}\label{Metodologías}
Durante el desarrollo del proyecto se ha seguido una metodología ágil, \textit{Scrum}, concretamente. El  inconveniente más evidente de esta filosofía en un proyecto educativo es la falta de un equipo de personas que cubran los diferentes roles necesarios. Sin embargo, he intentado ponerme en el papel de cada componente del equipo, haciéndome preguntas constantemente sobre el tiempo disponible, el producto final requerido en cada sprint y qué esperaría un potencial cliente. Cada \textit{issue} se describe detalladamente, especificando el objetivo.  

Estos son algunos de los procesos más relevantes que he seguido:

\begin{itemize}
\item Realización de iteraciones, \textit{sprints}, de forma constante. Los sprints han tenido una duración aproximada de unas dos semanas, aunque algunos se han incrementado debido a cambios en el objetivo final del proyecto y en la forma de actuar. Por ejemplo, en el \textit{sprints} 6, se cambió la lógica recoger las respuestas a las preguntas. Aquí he intentado realizar un esfuerzo extra en cuanto a lo esperable en las reuniones diarias y, aunque han sido discusiones conmígo mismo y con el tutor, el resultado ha sido satisfactorio. 
\item Disposición de tareas, \textit{Issues}, en cada uno de los sprints, estos son tareas específicas que se deben completar dentro del marco temporal del sprint. Esto asegura que todas las actividades necesarias para avanzar en el proyecto estén identificadas y asignadas, facilitando un seguimiento efectivo. Además, cada commit se ha referenciado a su \textit{Issues} correspondiente. 
    
\end{itemize}

\section{Extracción de la información}\label{Metodologías}

La primera fase de este proyecto ha sido la de extraer información. Ha sido la más extensa y, aunque ha sido un proceso continuo que se ha llevado a cabo a lo largo de todo el proyecto entendiendo el funcionamiento de los sistemas que proporcionan, ha tenido lugar en el primer \textit{Sprint}.

Para esto, es necesario tener en cuenta varias bases de datos bibliográficas que ofrecen diversos recursos para los investigadores. Entre las más importantes se encuentran Google Scholar  y Crossref. En ellas he podido encontrar diferentes artículos científicos de proyectos sencillos utilizando los servicios, aunque ninguno era lo suficientemente complejo, me ha servido para entenderlos por encima. \\TODO insertar referencias.

Sin embargo, donde más he podido obtener información ha sido en la documentación de ambas plataformas. Las dos proporcionan una gran cantidad de documentación para todos sus servicios, cubriendo desde conceptos básicos hasta detalles técnicos avanzados.

A pesar de la abundancia de información disponible en ambas plataformas, encontré que la documentación de AWS es mucho más sencilla. Esto fue particularmente útil en las primeras etapas del proyecto, cuando estaba familiarizándome con los diferentes componentes y cómo integrarlos entre ellos.

Por otro lado, la documentación de Google Cloud Platform, aunque igualmente completa y detallada, a veces se adentra en temas muy técnicos y específicos que no eran necesarios para mi proyecto. Esto hizo que tuviera que invertir más tiempo en filtrar la información y extraer solo lo más relevante para las necesidades del desarrollo del chatbot.


\section{Amazon Web Services}\label{AWS}
\imagen{AWS.png}{Ecosistema AWS. Fuente: elaboración propia}{1}

\subsection{Qué elementos contienen}\label{elementos-aws}
\begin{enumerate}

    \item Amazon S3 (Simple Storage Service).
    
    Es un servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos y seguridad. Los objetos se almacenan en contenedores llamados buckets. 
    
    En mi caso, almaceno los archivos de entrada y los resultados del procesamiento. Hay dos buckets: uno para los archivos a procesar \textbf{(archivos-a-procesar)} y otro para los resultados procesados \textbf{(resultados-textract)}.
    \item AWS Lambda.

    AWS Lambda es un servicio de computación serverless que permite ejecutar código en respuesta a eventos sin tener que aprovisionar ni gestionar servidores. Se paga solo por el tiempo de computación consumido. Son tres las funciones que utilizo:
    \begin{itemize}
        \item \textbf{invokeTextract:} Esta función invoca Amazon Textract para comenzar la extracción de texto de los documentos almacenados en S3 obteniendo el ID de la extracción.
        \item \textbf{ResultTextract:} Esta función procesa las notificaciones de finalización de Amazon Textract, obteniendo el texto extraído para luego manejar los resultados, almacenándolos en S3.
        \item \textbf{LexIntegration:} Esta función integra el chatbot con Amazon Lex y maneja la lógica adicional necesaria, interactuando con DynamoDB.
    \end{itemize}

    \item Amazon Textract.

    Amazon Textract es un servicio que utiliza machine learning para extraer texto y datos de documentos escaneados automáticamente. Puede leer archivos escaneados, identificar campos de formulario y tablas.

    Este servicio es invocado por las funciones invokeTextract y ResultTextract para obtener el texto extraído de archivos pdf en formato txt. De esta forma es posible su posterior uso en el bot.
    
    \item Amazon SNS (Simple Notification Service)

    Amazon SNS es un servicio de mensajería que coordina y gestiona la entrega de mensajes a suscriptores en diferentes plataformas y dispositivos.  Envía una notificación cuando Amazon Textract completa el procesamiento de un documento, activando la función Lambda ResultTextract que extrae y procesa los resultados.
    
    \item Amazon Comprehend

    Amazon Comprehend es un servicio de procesamiento del lenguaje natural (NLP) que utiliza machine learning para encontrar insights y relaciones en el texto. Ofrece capacidades como análisis de sentimientos, detección de entidades, análisis de lenguaje y clasificación de texto. Es usado por la función ResultTextract para detectar el idioma que está escrito el texto. 
    \item Amazon Translate

    Amazon Translate es un servicio de traducción automática que proporciona traducciones rápidas y de alta calidad entre diferentes idiomas. Utiliza técnicas de machine learning para traducir el texto.

    Este traduce el texto a español en caso de que el idioma detectado por Comprehend sea otro distinto al español, permitiendo así que el bot reciba documentos en cualquier idioma.
    
    \item Amazon Lex

    Amazon Lex es un servicio para construir interfaces de conversación utilizando voz y texto. Utiliza técnicas avanzadas de comprensión del lenguaje natural (NLU)\\TODO poner referencia. para entender la intención del usuario y gestionar diálogos de manera dinámica.

    Este desarrolla el chatbot que interactúa con los usuarios, obteniendo las preguntas que realizan y utilizando los datos procesados y almacenados en DynamoDB para proporcionar las respuestas. 
    \item Amazon DynamoDB

    Amazon DynamoDB es un servicio de base de datos NoSQL que ofrece rendimiento rápido y predecible con escalabilidad automática. En él se encuentra la base de datos con todas las respuestas a las preguntas del usuario. Dispone de tres campos: Nombre del intent reconocido por Lex (IntentName), Texto de la pregunta (Question), Texto de la respuesta (Response).
\end{enumerate}
\subsection{Funcionamiento}\label{funcionamiento-aws}
El flujo de trabajo para el desarrollo del chatbot se compone de varias etapas, son dos las secciones que pueden actuar independientemente y que se pueden diferenciar de la siguiente forma:  

\begin{itemize}
    \item Sección 1: Extracción y Procesamiento de Documentos: Esta se encarga únicamente de la tarea de extracción del texto.
    \item Sección 2: Interacción del Chatbot: No es necesario que haya texto extraído dentro del bucket para funcionar, ya que actúa independientemente mediante la base de datos. 
\end{itemize}

A continuación se detalla cada paso del proceso:

\subsubsection{Sección 1: Extracción y Procesamiento de Documentos}

\begin{enumerate}
    \item \textbf{Carga de Archivos en S3}. 
    
    Dentro del bucket y la carpeta \textit{resultados-a-procesar/} se cargan los archivos con extensión .pdf para extraerles el texto y poder trabajar con ellos. Nada más cargarse el archivo pdf, desencadena la función invokeTextract. 
    
    \item \textbf{Desencadenación de invokeTextract}. 
    
    Una vez que ha recibido la notificación por parte del bucket S3, esta función se activa. Recibe el nombre del bucket y el archivo que se ha cargado. A continuación, utiliza la función de textract start\_document\_text\_detection para obtener el ID del proceso que inicia la extracción. 

    \item \textbf{Notificación SNS}
    Tras obtener el ID, utiliza el servicio de SNS para notificar a la función ResultTextract que puede continuar con la extracción del texto. Esto activa a la función que recibirá el ID por parte de SNS.

    Los siguientes pasos serán realizados por la función ResultTextract y pueden ser agrupados como el procesamiento de los datos

    \item \textbf{Desencadenación de ResultTextract}.

    Esta función lambda recibe la notificación con el ID, comienza la extracción del texto. \\TODO introducir diagrama de flujo y explicar mejor

    \item \textbf{Traducción del texto}
    La función ResultTextract utiliza el servicio Amazon Comprehend para detectar el idioma del texto extraído. En caso de que el idioma sea distinto al español, llama al servicio Amazon Translate para traducirlo, por lo que este paso es opcional. 

    \item \textbf{Carga de los datos}
    Una vez la función ResultTextract ha terminado toda la tarea de preprocesamiento de datos, carga el archivo en formato txt con el texto extraído dentro del bucket y la carpeta resultados-textract/.
    
\end{enumerate}

\subsubsection{Sección 2: Interacción del Chatbot}
\begin{enumerate}[7.]
    \item \textbf{LexIntegration}
    Esta función es la que maneja toda la sección 2. Cada vez que Amazon Lex detecta un intent, llama a la función lambda quien determina como seguir. \\TODO continuar explicando
    
\end{enumerate}

\subsection{Montaje}\label{montaje-aws}
En esta sección, se detallará cómo se ha montado todo el entorno para el desarrollo del chatbot utilizando los servicios de AWS.

\subsubsection{Creación del Rol IAM}
Para comenzar, es necesario configurar un Rol IAM en AWS. Este rol define los permisos que las funciones Lambda necesitarán para interactuar con otros servicios de AWS.

Primero, busca el servicio Identity and Access Management (IAM) y crea un nuevo rol en la sección Roles. Yo le he puesto el nombre “AWS\_PDF\_Textract\_Lex\_Role”. En el Paso 1, configura el tipo de entidad de confianza como Servicio de AWS, con el caso de uso Lambda. En el Paso 2, asigna los siguientes permisos: \texttt{AmazonS3FullAccess}, \texttt{AmazonTextractFullAccess}, \texttt{AWSLambdaExecute}, \texttt{AWSLambdaSNSFullAccess}, \texttt{ComprehendFullAccess} y \texttt{TranslateFullAccess}. En el Paso 3, elige un nombre para el rol y revisa el resumen de configuración. Finalmente, crea el rol.

Repite estos pasos para crear otro rol con los siguientes permisos, en mi caso lo llamé “AWS\_Lambda\_LexIntegration”: \texttt{AmazonDynamoDBFullAccess}, \texttt{AmazonS3FullAccess}, \texttt{AWSLambdaBasicExecutionRole} y \texttt{AmazonLexFullAccess}.

\subsubsection{Creación de un Bucket S3 con dos Carpetas}
El siguiente paso es crear un bucket en Amazon S3 para almacenar los archivos que se procesarán y los resultados. Busca el servicio Amazon S3 y crea un nuevo bucket, eligiendo un nombre y dejando todas las configuraciones por defecto para mayor seguridad. Dentro de este bucket, crea dos carpetas: una para cargar los PDF y otra para almacenar el texto extraído de los PDF en formato TXT.

\subsubsection{Crear un SNS}
Para la mensajería y las notificaciones, utiliza Amazon Simple Notification Service (SNS). Busca el servicio SNS y crea un nuevo tópico, asignándole un nombre y seleccionando el tipo estándar. Deja todas las configuraciones por defecto y crea el tópico.

\subsubsection{Creación de las Funciones Lambda}
En este paso, se crearán tres funciones Lambda que interactuarán con S3, Textract y Lex.

Primero, busca el servicio AWS Lambda y crea una nueva función desde cero. Asigna un nombre a la función y selecciona Python como el lenguaje de tiempo de ejecución, con arquitectura x86\_64. En la sección de permisos, añade el rol IAM “AWS\_PDF\_Textract\_Lex\_Role”. Después de crear la función, ve a la configuración básica y ajusta la memoria a 1GiB, el almacenamiento efímero y el tiempo de espera a 15 minutos. Configura un desencadenador (trigger) para el bucket S3 que hemos creado, especificando el prefijo de la carpeta donde se cargarán los PDF y el sufijo \texttt{.pdf}.

La segunda función Lambda será similar a la primera, pero el desencadenador será el servicio SNS configurado anteriormente. Utiliza el mismo rol IAM que la primera función.

La tercera función Lambda no tendrá un desencadenador y puedes usar la misma configuración básica de las funciones anteriores, exceptuando el rol, que será el “AWS\_Lambda\_LexIntegration”.

\subsubsection{Creación de la Base de Datos}
Ahora vamos a configurar la base de datos donde el chatbot buscará respuestas a las preguntas. Busca el servicio Amazon DynamoDB y crea una nueva tabla. Introduce el nombre de la tabla y establece la clave de partición como \texttt{IntentName}. Esto permitirá recuperar el conjunto de preguntas asociadas a cada intent y proporcionar respuestas precisas. Deja el resto de las configuraciones por defecto y crea la tabla.

Después, crea una función Lambda para cargar los datos en la tabla DynamoDB. El código necesario para esta función se puede encontrar en un enlace de GitHub especificado. Una vez creada la función, realiza una prueba para asegurarte de que los datos se cargan correctamente.

\subsubsection{Creación del Bot}
Para configurar el chatbot, accede al servicio AWS Lex y crea un nuevo bot desde cero. Asigna un nombre al bot y deja la configuración de permisos predeterminada, ya que las funciones Lambda previamente creadas serán las que interactúen con los servicios necesarios.

Crea una lista de intents y añade ejemplos de enunciados para cada uno. Configura los hooks de código utilizando las funciones Lambda creadas anteriormente. Los intents iniciales que debes crear incluyen \texttt{StartTutorial}, \texttt{RepeatStep}, \texttt{NextStep} y \texttt{GoToStep}. Añade frases de entrenamiento como “crear un chatbot” o “siguiente paso”.

A continuación, configura los intents correspondientes al banco de preguntas. Asegúrate de utilizar los nombres de intents que coincidan con los utilizados en la función Lambda que carga los datos en DynamoDB, para garantizar la correcta detección y respuesta. Añade también las frases de entrenamiento similares a las preguntas que has introducido en la base de datos.

\subsubsection{Implementación del Chatbot}
\\TODO


\subsection{Costos y escalabilidad}\label{costos-aws}


\section{Google Cloud}\label{GCP}
\imagen{GCP.png}{Ecosistema GCP. Fuente: elaboración propia}{1} 
\subsection{Qué elementos contienen}\label{elementos-gcp}
\begin{enumerate}

    \item Cloud Storage
    
    Cloud Storage es un servicio de almacenamiento de objetos de Google Cloud que ofrece escalabilidad, disponibilidad de datos y seguridad. Los objetos se almacenan en contenedores llamados buckets. 
    
    En mi caso, almaceno los archivos de entrada y los resultados del procesamiento. Hay dos buckets: uno para los archivos a procesar \textbf{(archivos-a-procesar)} y otro para los resultados procesados \textbf{(resultados-extraídos)}.
    
    \item Cloud Functions
    
    Cloud Functions es un servicio de computación sin servidor que permite ejecutar código en respuesta a eventos sin tener que aprovisionar ni gestionar servidores. Solo se paga por el tiempo de computación consumido. Son tres las funciones que utilizo:
    \begin{itemize}
        \item \textbf{documentAI-extract-text:} Esta función invoca Cloud Document AI para comenzar la extracción de texto de los documentos almacenados en Cloud Storage obteniendo el ID de la extracción.
        \item \textbf{analyze-text:} Esta función procesa las notificaciones de finalización de Cloud Document AI, obteniendo el texto extraído para luego manejar los resultados, almacenándolos en Cloud Storage.
        \item \textbf{DialogFlow-integration:} Esta función integra el chatbot con DialogFlow y maneja la lógica adicional necesaria, interactuando con Firestore.
    \end{itemize}
    
    \item Cloud Document AI
    
    Cloud Document AI es un servicio que utiliza machine learning para extraer texto y datos de documentos escaneados automáticamente. Puede leer archivos escaneados, identificar campos de formulario y tablas.
    
    Este servicio es invocado por las funciones documentAI-extract-text y analyze-text para obtener el texto extraído de archivos PDF en formato TXT, permitiendo su posterior uso en el bot.
    
    \item Cloud Tasks
    
    Cloud Tasks es un servicio de gestión de colas que coordina y gestiona la entrega de tareas a servicios de back-end de manera asíncrona. Envía una notificación cuando Cloud Document AI completa el procesamiento de un documento, activando la función analyze-text que extrae y procesa los resultados.
    
    \item Cloud Language
    
    Cloud Language es un servicio de procesamiento del lenguaje natural (NLP) que utiliza machine learning para encontrar insights y relaciones en el texto. Ofrece capacidades como análisis de sentimientos, detección de entidades, análisis de lenguaje y clasificación de texto. Es utilizado por la función analyze-text para detectar el idioma en el que está escrito el texto.
    
    \item Cloud Translate
    
    Cloud Translate es un servicio de traducción automática que proporciona traducciones rápidas y de alta calidad entre diferentes idiomas. Utiliza técnicas de machine learning para traducir el texto.
    
    Este servicio traduce el texto al español en caso de que el idioma detectado por Cloud Language sea otro distinto al español, permitiendo así que el bot reciba documentos en cualquier idioma.
    
    \item Cloud DialogFlow
    
    Cloud DialogFlow es un servicio para construir interfaces de conversación utilizando voz y texto. Utiliza técnicas avanzadas de comprensión del lenguaje natural (NLU) para entender la intención del usuario y gestionar diálogos de manera dinámica.
    
    Este servicio desarrolla el chatbot que interactúa con los usuarios, obteniendo las preguntas que realizan y utilizando los datos procesados y almacenados en Firestore para proporcionar las respuestas.
    
    \item Cloud Firestore
    
    Cloud Firestore es una base de datos NoSQL que ofrece rendimiento rápido y predecible con escalabilidad automática. En él se encuentra la base de datos con todas las respuestas a las preguntas del usuario. Dispone de tres campos: Nombre del intent reconocido por DialogFlow (IntentName), Texto de la pregunta (Question), Texto de la respuesta (Response).
    
\end{enumerate}

\subsection{Funcionamiento}\label{funcionamiento-gcp}
El flujo de trabajo para el desarrollo del chatbot se compone de varias etapas, son dos las secciones que pueden actuar independientemente y que se pueden diferenciar de la siguiente forma:  

\begin{itemize}
    \item Sección 1: Extracción y Procesamiento de Documentos: Esta se encarga únicamente de la tarea de extracción del texto.
    \item Sección 2: Interacción del Chatbot: No es necesario que haya texto extraído dentro del bucket para funcionar, ya que actúa independientemente mediante la base de datos. 
\end{itemize}

A continuación se detalla cada paso del proceso:

\subsubsection{Sección 1: Extracción y Procesamiento de Documentos}

\begin{enumerate}
    \item \textbf{Carga de Archivos en Cloud Storage}. 
    
    Dentro del bucket y la carpeta \textit{archivos-a-procesar/} se cargan los archivos con extensión .pdf para extraerles el texto y poder trabajar con ellos. Nada más cargarse el archivo pdf, desencadena la función documentAI-extract-text. 
    
    \item \textbf{Desencadenación de documentAI-extract-text}. 
    
    Una vez que ha recibido la notificación por parte del bucket Cloud Storage, esta función se activa. Recibe el nombre del bucket y el archivo que se ha cargado. A continuación, utiliza el servicio Cloud Document AI para iniciar la extracción de texto y obtener el ID del proceso.
    
    \item \textbf{Notificación Cloud Tasks}
    
    Tras obtener el ID, utiliza el servicio de Cloud Tasks para notificar a la función analyze-text que puede continuar con la extracción del texto. Esto activa la función que recibirá el ID por parte de Cloud Tasks.

    Los siguientes pasos serán realizados por la función analyze-text y pueden ser agrupados como el procesamiento de los datos

    \item \textbf{Desencadenación de analyze-text}.

    Esta función recibe la notificación con el ID, comienza la extracción del texto utilizando Cloud Document AI.

    \item \textbf{Traducción del texto}
    
    La función analyze-text utiliza el servicio Cloud Language para detectar el idioma del texto extraído. En caso de que el idioma sea distinto al español, llama al servicio Cloud Translate para traducirlo, por lo que este paso es opcional.

    \item \textbf{Carga de los datos}
    
    Una vez la función analyze-text ha terminado toda la tarea de preprocesamiento de datos, carga el archivo en formato txt con el texto extraído dentro del bucket y la carpeta \textit{resultados-extraídos/}.
    
\end{enumerate}

\subsubsection{Sección 2: Interacción del Chatbot}

\begin{enumerate}[7.]
    \item \textbf{DialogFlow-integration}
    
    Esta función es la que maneja toda la sección 2. Cada vez que Cloud DialogFlow detecta un intent, llama a la función Cloud Functions quien determina cómo seguir. La función interactúa con Cloud Firestore para obtener las respuestas almacenadas en la base de datos y las proporciona al usuario a través del chatbot. 

    Esta integración permite que el chatbot responda preguntas utilizando la información preprocesada y almacenada, asegurando que las respuestas sean precisas y relevantes, independientemente de si hay texto recién extraído en el bucket.

\end{enumerate}

\subsection{Montaje}\label{montaje-gcp}

Para comenzar a trabajar con los servicios de Google Cloud y crear el entorno del chatbot, es necesario seguir una serie de pasos detallados a continuación.

\subsubsection{Creación del Proyecto}
Lo primero que se debe hacer es crear un proyecto nuevo en la consola de Google Cloud. Al acceder a la consola, se presenta la opción de crear un nuevo proyecto. Es importante guardar el ID del proyecto, ya que puede ser necesario más adelante para configurar el entorno virtual.

\subsubsection{Paso 1: Creación del Storage}
El primer paso es crear un bucket en el servicio Cloud Storage. Se debe configurar el nombre del bucket y la región deseada, dejando el resto de las configuraciones en su estado predeterminado. Una vez creado el bucket, se deben crear dos carpetas dentro de él: una para los archivos PDF que se desean procesar y otra para almacenar los archivos TXT con el texto extraído.

\subsubsection{Paso 2: Creación de Document AI}
A continuación, se configura el servicio Document AI. Al acceder a este servicio, se solicita habilitar la API correspondiente. Después, se procede a crear un procesador, eligiendo el tipo Document OCR para realizar la extracción de texto. Se le asigna un nombre y una región al procesador. Es recomendable mantener la misma región para todos los servicios relacionados con el proyecto. Una vez configurado, se puede probar subiendo un documento de prueba para verificar su correcto funcionamiento.

\subsubsection{Paso 3: Creación de las Cloud Functions}
En este paso, se crean tres funciones Cloud Functions, cada una con un propósito específico: extraer texto, procesar el texto y gestionar la interacción con DialogFlow.

La primera función se encarga de la extracción de texto. Se selecciona el entorno de 1ª generación por su estabilidad. Se configura el nombre y la región, y se establece como activador Cloud Storage con tipo de evento "finalized". Se ajustan los recursos de memoria, CPU y tiempo de espera para asegurar la finalización de la tarea. También se asigna la cuenta de servicio del proyecto.

Antes de configurar las otras funciones, se crea una cola en el servicio Cloud Tasks para gestionar las notificaciones. Se habilita la API de Cloud Tasks y se configura el nombre y la región de la cola.

La segunda función se activa mediante una notificación HTTP enviada por la primera función. Esta función realiza la tarea de procesar el texto, para lo cual necesita habilitar las API de Cloud Natural Language y Cloud Translate.

La tercera función se encarga de la integración con el bot de DialogFlow. Esta función se configura de manera similar a las anteriores, pero con el propósito específico de gestionar la interacción con DialogFlow.

\subsubsection{Paso 4: Creación de la Base de Datos}
Para almacenar el banco de preguntas y respuestas del chatbot, se utiliza Firestore. Se selecciona el modo Nativo y se asigna un nombre, dejando el resto de configuraciones por defecto. Una vez creada la base de datos, se añaden dos colecciones: una para el banco de preguntas y respuestas (\texttt{chatbotresponses}) y otra para los pasos de creación del chatbot (\texttt{chatbotsteps}).

\subsubsection{Paso 5: Configuración de DialogFlow ES}
Se utiliza DialogFlow ES (Essentials) para configurar el bot. En la consola de DialogFlow, se crean los intents necesarios, añadiendo frases de entrenamiento y habilitando la opción "Enable webhook call for this intent" en el fulfilment.

Los intents iniciales incluyen \texttt{StartTutorial}, \texttt{RepeatStep}, \texttt{NextStep} y \texttt{GoToStep}, con frases de entrenamiento como “crear un chatbot” o “siguiente paso”. También se configuran los intents correspondientes al banco de preguntas, utilizando los nombres de intents que recibe la Cloud Function y añadiendo frases de entrenamiento similares a las preguntas almacenadas en la base de datos.

\subsubsection{Paso 6: Implementación del Chatbot}
Finalmente, se implementa el chatbot en una página web. En la consola de DialogFlow, se navega a la configuración del bot y se añade una descripción. En el apartado de “Integrations” -> “Web demo”, se copia el código \texttt{<iframe>} y se inserta en la página web.

\subsection{Costos y escalabilidad}\label{costos-gcp}


\section{Creación de la aplicación web}\label{webapp}

\section{Comparativa de servicios}\label{comparativa}
Problemas encontrados, soluciones, comparativa de respuestas?, cual me resultó mas facil, diferencias, Experiencia del Usuario (UX)